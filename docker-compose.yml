version: '3.7'

services:
  substrate-node:
    image: taulepton/reef:v10
    volumes:
      - substrate-data:/data
    ports:
      - ${EXPOSE_NODE_PORT}:${NODE_PORT}
      - ${EXPOSE_NODE_WS}:${NODE_WS}
    # TODO paramaterise chain
    command: -d /data --rpc-max-payload 1024 --ws-max-connections ${NODE_MAX_CONS} ${NODE_CHAIN_CMD} ${NODE_CHAIN} --pruning=archive --rpc-cors "*" --ws-port ${NODE_WS} --port ${NODE_PORT} --unsafe-ws-external --no-prometheus --no-telemetry
    restart: always
    networks:
      - reefscan

  postgres:
    image: postgres:13
    command: postgres -c shared_preload_libraries=pg_stat_statements -c pg_stat_statements.track=all
    restart: always
    shm_size: 1gb
    volumes:
      - 'db-data:/var/lib/postgresql/data'
    ports:
      - ${EXPOSE_DB}:5432
    environment:
      POSTGRES_USER: ${DB_USER} #'reefexplorer'
      POSTGRES_PASSWORD: ${DB_PW} #'reefexplorer'
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U reefexplorer']
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - reefscan

  graphql:
    image: hasura/graphql-engine:v2.1.1.cli-migrations-v3
    ports:
      - ${EXPOSE_GQL_PORT}:8080
    volumes:
      - './db/hasura/metadata:/hasura-metadata'
      - './db/hasura/migrations:/hasura-migrations'
    depends_on:
      - 'postgres'
    restart: always
    environment:
      HASURA_GRAPHQL_DATABASE_URL: ${DB_URL} #postgres://reefexplorer:reefexplorer@postgres:5432/reefexplorer
      HASURA_GRAPHQL_ENABLE_CONSOLE: ${GQL_CONSOLE} #'true' # set to "false" to disable console
      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log
      HASURA_GRAPHQL_UNAUTHORIZED_ROLE: ${GQL_UNAUTHORIZED_ROLE} #public
      HASURA_GRAPHQL_ADMIN_SECRET: ${GQL_ADMIN_PW} #myadminsecretkey
      HASURA_GRAPHQL_V1_BOOLEAN_NULL_COLLAPSE: "true"
    networks:
      - reefscan

  crawler:
    image: reef-explorer-crawler-dev
    build:
      context: .
      args:
        COMPONENT: crawler
    volumes:
      - crawler-modules:/usr/app/node_modules
      - ./crawler:/usr/app/
    restart: always
    environment:
      - NODE_PROVIDER_URLS=["ws://substrate-node:9944"] #TODO extract this list
      - START_BLOCK_STEP=32
      # with 8 nodes you can process up to 4096 blocks at once but rather keep it at 1024 if server doesn't have a lot of RAM.
      # Currently we support node with 16Gb of RAM. To change the capacity go in `./crawler/package.json` and change --max-old-space-size=16384 to desired size
      - MAX_BLOCKS_PER_STEP=32
      - SENTRY_DNS=${SENTRY_DNS}
      - POSTGRES_PORT=${DB_PORT} #5432
      - POSTGRES_HOST=${DB_HOST}
      - POSTGRES_USER=${DB_USER} #reefexplorer
      - POSTGRES_DATABASE=${DB_NAME} #reefexplorer
      - POSTGRES_PASSWORD=${DB_PW} #reefexplorer
      - ENVIRONMENT=${ENVIRONMENT}
      - NETWORK=${NETWORK}
      - SUBCONTRACT_INTERVAL=${SUBCONTRACT_INTERVAL}
      - LIVE_GRAPHQL_URL=${LIVE_GRAPHQL_URL}
      - VERIFIED_CONTRACT_SYNC=${VERIFIED_CONTRACT_SYNC}
      - VERIFIED_CONTRACT_SYNC_INTERVAL=${VERIFIED_CONTRACT_SYNC_INTERVAL}
    depends_on:
      - 'postgres'
      - 'substrate-node'
    networks:
      - reefscan

  backtracking:
    image: reef-explorer-crawler-dev
    command: /wait-for http://graphql:8080/healthz -t 0 -- yarn dev-backtracking
    volumes:
      - backtracking-modules:/usr/app/node_modules
      - ./crawler:/usr/app/
    restart: always
    environment:
      - NODE_PROVIDER_URLS=["ws://substrate-node:9944"] #TODO extract this list
      - SENTRY_DNS=${SENTRY_DNS}
      - POSTGRES_PORT=${DB_PORT} #5432
      - POSTGRES_HOST=${DB_HOST}
      - POSTGRES_USER=${DB_USER} #reefexplorer
      - POSTGRES_DATABASE=${DB_NAME} #reefexplorer
      - POSTGRES_PASSWORD=${DB_PW} #reefexplorer
      - ENVIRONMENT=${ENVIRONMENT}
      - NETWORK=${NETWORK}
    depends_on:
      - 'postgres'
      - 'substrate-node'
    networks:
      - reefscan

  pool:
    image: reef-explorer-crawler-dev
    command: /wait-for http://graphql:8080/healthz -t 0 -- yarn dev-pool
    volumes:
      - pool-modules:/usr/app/node_modules
      - ./crawler:/usr/app/
    restart: always
    environment:
      - FACTORY_ADDRESS=${FACTORY_ADDRESS}
      - NODE_PROVIDER_URLS=["ws://substrate-node:9944"] #TODO extract this list
      - SENTRY_DNS=${SENTRY_DNS}
      - POSTGRES_PORT=${DB_PORT} #5432
      - POSTGRES_HOST=${DB_HOST}
      - POSTGRES_USER=${DB_USER} #reefexplorer
      - POSTGRES_DATABASE=${DB_NAME} #reefexplorer
      - POSTGRES_PASSWORD=${DB_PW} #reefexplorer
      - ENVIRONMENT=${ENVIRONMENT}
      - NETWORK=${NETWORK}
      - POOL_CONTRACT_VERIFICATION_AFTER_BLOCK=${POOL_CONTRACT_VERIFICATION_AFTER_BLOCK}
    depends_on:
      - 'postgres'
      - 'substrate-node'
    networks:
      - reefscan

  api:
    image: reef-explorer-api-dev
    build:
      context: .
      args: 
        COMPONENT: api
    volumes:
      - api-modules:/usr/app/node_modules
      - ./api:/usr/app/
    restart: always
    environment:
      - PORT=${API_PORT} #8000
      - NODE_URL=ws://${NODE_HOST}:${NODE_WS}
      - POSTGRES_PORT=${DB_PORT}
      - POSTGRES_HOST=${DB_HOST}
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_DATABASE=${DB_NAME}
      - POSTGRES_PASSWORD=${DB_PW}
      - RECAPTCHA_SECRET=6LfNcPIaAAAAADSldnLXXxSrXIYH532l0cSsfDEU
      - SENTRY_DNS=${SENTRY_DNS}
      - ENVIRONMENT=${ENVIRONMENT}
      - NETWORK=${NETWORK}
    ports:
      - ${EXPOSE_API_PORT}:${API_PORT}
    depends_on:
      - postgres
      - substrate-node
    networks:
      - reefscan

  frontend:
    build: 
      context: .
      args:
        COMPONENT: frontend
    image: reef-explorer-frontend-dev
    volumes:
      - frontend-modules:/usr/app/node_modules
      - ./frontend:/usr/app/
    ports:
      - ${EXPOSE_FE_PORT}:${FE_PORT}
    environment:
      - HOST=0.0.0.0
    networks:
      - reefscan


# Persistent volumes
volumes:
  db-data: {}
  substrate-data: {}
  crawler-modules: {}
  backtracking-modules: {}
  pool-modules: {}
  api-modules: {}
  frontend-modules: {}

networks:
  reefscan:
